{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# CUS615 Demo code on SVM classifier \n",
    "This file includes a sample code on the materials we discussed in class. Use it as a playground to try the different methods. You can use it as a reference to implement your assignment.\n",
    "\n",
    "The code below demonstrates the following\n",
    "* loads a sample dataset in 2D feature spaces,\n",
    "* splits the dataset into training and testing sets, \n",
    "* inspects the data characteristics/statistics.\n",
    "* trains a SVM classifier on the training dataset.\n",
    "* applied the trained dataset of the testing set and makes predictions. \n",
    "* uses the ground truth label of the testing set to evaluate the classifier performance using\n",
    "    * the confusion matrix.\n",
    "    * the Normalized confusion matrix. \n",
    "    * Accuracy, precision, recall and f1-score.\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update path to include root folder of current directory. This is needed in order to import cus615 utilitis. \n",
    "import sys\n",
    "sys.path.insert(0, './../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import datasets\n",
    "from sklearn import svm\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn import metrics\n",
    "from cus615_utils.cus615_utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data -  \n",
    "#\n",
    "n_samples = 1500\n",
    "noise_level = 0.35    # set noise level from 0.0 to 0.35\n",
    "noisy_circles = datasets.make_circles(n_samples=n_samples, factor=.5, noise=noise_level)\n",
    "X, y = noisy_circles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split dataset into training and testing sets. \n",
    "x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=45)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the data since they are in 2D\n",
    "showDataPoint2D(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Look at the data.\n",
    "#\n",
    "n_samples_train, n_features = x_train.shape\n",
    "n_samples_test, _ = x_test.shape\n",
    "n_classes = len(np.unique(y_train))\n",
    "\n",
    "print(\"Number of samples in training set: %d ( %d possitive, %d negative)\" % (n_samples_train,np.sum(y_train==1),np.sum(y_train==0)))\n",
    "print(\"Number of samples in the testing set: %d (%d possitive, %d negative)\" % (n_samples_test,np.sum(y_test==1),np.sum(y_test==0)))\n",
    "print(\"Number of features: \" +  str(n_features))\n",
    "print(\"Number of classes: \" + str(n_classes))\n",
    "print(\"IDs for class labels: \" + str(np.unique(y_train)))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define SCM model parameters\n",
    "Soft-margin SVM introduces a __penalty__ term when maximizing boundary margin. The value of this penalty term is a parameter that the user can specified. In sklearn implementation of SVM, the user can specify this parameter $C$ relaxes the requirement for the classifier the correctly classify all points in the training data set. \n",
    "\n",
    "Various kernel depend of different parameters. For example, the `poly` kernel depends on the parameter `d` which specified the degree of the polynomial and `rbf` kernel depends on a parameter $\\gamma =1/\\sigma^2$. In sklearn implementation the user can specify these parameters when instantiating the classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spacify the classifier \n",
    "# Try different values for parameter C.\n",
    "# Try different values of parameter kernel : 'linear', 'poly', 'rbf'\n",
    "# Try different values for parameter gamma (for supported kernels).\n",
    "# Check the the classification boundary changes for different gammas. \n",
    "#\n",
    "#\n",
    "model = svm.SVC(C=4.4, kernel='rbf',probability=False,gamma=12.5)\n",
    "\n",
    "model = svm.SVC(C=4.4, kernel='rbf',probability=False,gamma=22.5)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model using the training sets\n",
    "model.fit(x_train,y_train)\n",
    "\n",
    "#Predict Output\n",
    "y_pred = model.predict(x_test)  \n",
    "\n",
    "y_true = y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute confusion matrix [Row: True Labels, Cols: Predicted]\n",
    "# from sklearn.metrics import confusion_matrix\n",
    "# Ref: https://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html\n",
    "\n",
    "print(\"\\n This is the confusion matrix\")\n",
    "cnf_mx = metrics.confusion_matrix(y_true, y_pred)\n",
    "print(cnf_mx)\n",
    "# Rows are y_true, Cols as y_false\n",
    "\n",
    "print(\"\\n This is the normalized confusion matrix.\")\n",
    "cnf_mx_joint = cnf_mx.astype('float')/ cnf_mx.sum()\n",
    "print(cnf_mx_joint)\n",
    "\n",
    "\n",
    "# Draw a graph with the onfusion matrix. \n",
    "plt.figure()\n",
    "class_names = {'Negative', 'Positive'}\n",
    "plot_confusion_matrix(cnf_mx, classes=class_names, title='Confusion matrix, without normalization')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Accuracy.\n",
    "#\n",
    "acc = metrics.accuracy_score(y_true, y_pred)\n",
    "# Display the output\n",
    "print(\"Accuracy: %.3f\" % acc)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Calcualte precision, positive predictive value. .\n",
    "# \n",
    "# The precision is the ratio tp / (tp + fp)\n",
    "#\n",
    "# where tp is the number of true positives and fp the number of false positives. \n",
    "#\n",
    "# The precision is intuitively the ability of the classifier not to label as positive a sample that is negative.\n",
    "# Alternativly, it denotes the probability that (1 denoting the positive class):\n",
    "#\n",
    "#      P(y_true=1| y_pred=1) \n",
    "#\n",
    "# REf: https://scikit-learn.org/stable/modules/generated/sklearn.metrics.precision_score.html#sklearn.metrics.precision_score\n",
    "\n",
    "precision = metrics.precision_score(y_true,y_pred, pos_label=0)  \n",
    "print(\"Precision (for class y=0 as positive class): %.3f\" % precision)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Calculating Recall - TPR \n",
    "# Sensitivity, recall, hit rate, or true positive rate (TPR)\n",
    "#\n",
    "# The recall is given by the ration  tp/(tp + fn)\n",
    "# \n",
    "# The recall is the ratio tp / (tp + fn) \n",
    "#     where tp is the number of true positives and fn the number of false negatives. \n",
    "#\n",
    "#  The recall is intuitively the ability of the classifier to find all the positive samples.\n",
    "#  Alternativly, it denotes the probability that (1 denoting the positive class):\n",
    "#\n",
    "#      P(y_pred=P |  y_true=P ) \n",
    "# Ref: https://scikit-learn.org/stable/modules/generated/sklearn.metrics.recall_score.html#sklearn.metrics.recall_score\n",
    "\n",
    "recall = metrics.recall_score(y_true, y_pred,pos_label=0)\n",
    "print(\"1. Recall/TPR (for class y=0 as positive class): %.3f\" % recall)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " \n",
    "# Calculating f1-score. \n",
    "# \n",
    "# The F1 score can be interpreted as a weighted average of the precision and recall, where an \n",
    "#     F1 score reaches its best value at 1 and worst score at 0. \n",
    "#     The relative contribution of precision and recall to the F1 score are equal. \n",
    "#     The formula for the F1 score is:\n",
    "#         \n",
    "#       2*(precision*recall)/(precision + recall)\n",
    "#   \n",
    "# Ref : https://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html#sklearn.metrics.f1_score\n",
    "\n",
    "f1_score = metrics.f1_score(y_true,y_pred, pos_label=0)\n",
    "print(\"1: F1_score (for class y=0 as positive class): %.3f\" % f1_score )\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Scikit learn provides a method to summarize the classification performance.\n",
    "# The method returns table that summarized 'precision', 'recall', 'f1-score'  and 'support'\n",
    "#\n",
    "\n",
    "target_names = ['Negative', 'Positive']\n",
    "print(metrics.classification_report(y_true,y_pred,target_names=target_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualizing the results on a 2D Classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display training results.\n",
    "plt.figure(figsize=(10,10))  # just to specify the figure size \n",
    "\n",
    "showDecisionBoundary2D(model)                 # Show the boundary region\n",
    "showDataPoint2D(x_test,y_test)  # Overlay the datapoints\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
